\chapter{Brownian Motion}
\begin{defi}
Let $(E, \Sigma)$ be a measurable space and $T$ a set.
A collection of $(E, \Sigma)$-valued random variables (RVs)
$\X = (X_t)_{t\in T}$ is called \defn{$E$-valued stochastic process (SP) with index set $T$}.
\end{defi}
%TODO Kasten

\begin{bsp}
We introduce three particularly interesting special cases of SPs.
\begin{enumerate}[label=(\alph*)]
\item An SP  with $(E, \Sigma) = (\Rd, \Borel(\Rd))$
	and $T=\R_{\geq 0}$
	is called \defn{$\Rd$-valued, continuous-time SP}.
\item For $E = \{-1,1\}$, \(\Sigma = \Po(E)\) and $T=\Z^d$,
	the SP is called \defn{spin system}.
\item If $E$ is countable and $T=\N_0$, we speak of a \defn{time discrete SP}.
\qedhere
\end{enumerate}
\end{bsp}

From a dynamical point of view,
$X_t$ is a $t$-dependent quantity that changes with time.
This perspective is suitable for the comprehension of the first and third example.
From a global point of view,
an SP is a single RV with values in the space $\Omega=E^T=\{f\colon T\to E\}$.
In the first example,
this means to consider the whole \defn{path} $(X_t)_{t\geq 0}$ as one object.
In the second example,
each spin configuration in $\{-1,1\}^{\Z^d}$ is an element of $\Omega$.
This raises some questions:
What is the “right” $\sigma$-algebra on \(\Omega\)?
Does it even exist?

\begin{defi}
Let $\X = (X_t)_{t\in T}$ be an SP
where the state space $E$ is a group, \eg $E=\Rd$,
and $T\subseteq \R$.
The family $(X_{s,t})_{s,t\in T}$ with $X_{s,t}\coloneqq X_t-X_s$
is called the \defn{increment process of $\X$} or \defn{set of increments}.

An SP has \defn{independent increments}
if for all $n\in \N$ and all
$s_1 < t_1 \leq s_2 < t_2 \leq \dots \leq s_n < t_n$
with $s_i, t_i \in T$,
the RVs $(X_{s_i,t_i})_{1\leq i \leq n}$ are independent.

An SP has \defn{stationary increments}
if for all $n\in \N$, all \(r \in T\) and all
$s_1 < t_1 \leq s_2 < t_2 \leq \dots \leq s_n < t_n$
with $s_i,t_i \in T$,
we have
\begin{align*}
(X_{s_i,t_i})_{i=1,\dots,n}\sim  (X_{s_i+r,t_i+r})_{i=1,\dots,n},
\end{align*}
\ie that the increments are equal in distribution
regardless at which time we started looking.
\end{defi}
%TODO Kasten

\begin{defi}
An $\Rd$-valued SP $\B=(B_t)_{t \in \R_{\geq 0}}$
is called \emph{Brownian Motion} (BM) if
\begin{enumerate}[label=(B\arabic*)]
\item $B_0(\omega)=0$ for almost all $w\in \Omega$,
\item $\B$ has independent increments,
\item $\B$ has stationary increments,
\item the increments are normally distributed, \ie
	\[B_{s,t} \coloneqq B_t - B_s \sim B_{t-s}\sim \Ndist{0}{(t-s)\Id},\]
	and
\item the map $t\mapsto B_t(\omega)$ is continuous for all $\omega \in \Omega$.
\qedhere
\end{enumerate}
\end{defi}
%TODO Kasten

In (B4) we require continuity of all paths.
For many settings,
it would be more natural to require this only for almost-all paths.
However, the set of continuous paths is usually not measurable
and it is often easier to work exclusively with the continuous paths.
It does not make much difference for the theory
but we will consistently work with this definition.

\begin{bem}
Checking the requirements (B0)\,--\,(B4)
in view of $B_t=\int_0^t \xi_s~\mathrm{d}s$:
\begin{enumerate}[label=(B\arabic*)]
\item $\int_0^0\xi_s~\mathrm{d}s=0$
\item $B_t-B_s=\int_s^t \xi_r~\mathrm{d}r$
	and $\xi_r \amalg (\xi_s)_{s\not =r}~~\forall r$
% TODO: Is this correct?
%	and the \(\xi_r\) are independent from each other
\item The distribution of $\xi_r$ does not depend on $r$.
\item Central limit theorem and Riemann approximation.
\item The map $t \mapsto \int_0^t f_s~\mathrm{d}s$ is continuous
	for all “sensible” functions $f$,
	in particular for $f = \xi$.
\qedhere
\end{enumerate}
\end{bem}

\begin{defi}
The \defn{Gaussian measure} $\Ndist{m}{\sigma^2}$
with mean $m$ and variance $\sigma^2$ is
the probability measure on $(\R,\mathcal{B}(\R))$ with Lebesgue density
\begin{align*}
	g_{m, \sigma^2}(x) &= \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2\sigma^2}(x-m)^2\right).
	\qedhere
\end{align*}
\end{defi}

\begin{prop}
Let $X\sim \Ndist{m}{\sigma^2}$.
Then:
\begin{enumerate}[label=(\alph*)]
\item $\E{X } =m$, $\V{X} = \sigma^2$
\item We have the \defn{Gaussian tail estimate}
	% TODO: Refactor terms?
	\begin{align*}
	\frac{1}{\sqrt{2\pi}} \frac{C}{C^2+1}\e^{-\frac{C^2}{2}}
	\leq
	\mathds{P}(X-m\geq C \sigma)
	\leq
	\frac{1}{\sqrt{2\pi}} \frac{1}{C}\e^{-\frac{C^2}{2}},
	\end{align*}
	for all $C>0$, $\sigma>0$.
\item For $(m_k)_{k\in \N}\subseteq \R$, $m\in \R$,
	$(\sigma_k)_{k\in \N}\subseteq \R_{\geq 0}$ and $\sigma\in \R_{\geq 0}$
	we have that
	$(m_k, \sigma_k) \to (m, \sigma)$
	if and only if
	$\Ndist{m_k}{\sigma_k^2} \distto  \Ndist{m}{\sigma^2}$.
\qedhere
\end{enumerate}
\end{prop}

\begin{defi}
An $\Rd$-valued RV $X$
is called \defn{$d$-dimensional Gaussian}
if for all linear functionals $L\colon \Rd\to \R$
there are $m$, $\sigma^2$ with $LX \sim \Ndist{m}{\sigma^2}$.
Explicitly:
If $\X=(X^1,\dots,X^d)$,
this means that for all $a_1,\dots,a_d \in \R$
there are $m$, $\sigma^2$
such that $\sum_{i=1}^d a_i X^i \sim \Ndist{m}{\sigma^2}$.
\end{defi}

\makeatletter
\begin{bsp}
\begin{enumerate}[label=(\alph*)]
\item If $X^1,\dots,X^d$ are independent $1$-dimensional Gaussian,
	then $\textbf{X}=(X^1,\dots,X^d)$ is $d$-dimensional Gaussian.
\item % FIXME: There has to be a better way.
	{\makeatletter\edef\@currentlabelname{(\thechapter.\arabic{ams@defi} \alph{enumi})}\label{counterexample_gauss_process}\makeatother}
	\emph{Warning:}
	Without independence, this is not true in general.
	Consider $X^1\sim \Ndist01$ and
	\begin{align*}
	X^2(\omega) = \begin{cases}
		-X^1(\omega), &\text{if \(\abs{X^1(\omega)}\leq 1\)},\\
		+X^1(\omega), &\text{if \(\abs{X^1(\omega)}>1\)}.
	\end{cases}
	\end{align*}
	Then, $X^2\sim \Ndist01$
	(to check this, compute $\mathds{P}(X^2<c)$ for all $c\in \R$)
	but $(X^1,X^2)$ is not Gaussian as
	$\abs{X^1(\omega)-X^2(\omega)}\leq 2$ for all $\omega \in \Omega$ and
	$\abs{X^1(\omega)-X^2(\omega)}\not \equiv 0$,
	which implies that $X^1-X^2$ is not Gaussian.
\qedhere
\end{enumerate}
\end{bsp}

\begin{ex}
	Are there pairwise independent
	\(X^1, \dots, X^d \sim \Normaldistribution\)
	such that \(\X = (X^d, \dots, X^d)\) is not Gaussian?
\end{ex}

\begin{prop}
A real RV $X$ is $\Ndist{m}{\sigma^2}$-distributed if and only if
its characteristic function is given by
\begin{align*}
	\varphi_X(u)
	&\overset\textast= \e^{\i um}\e^{-\frac{1}{2}\sigma^2u^2}
	= \exp\autol(\i um - \frac12\sigma^2u^2\autor).
\qedhere
\end{align*}
\end{prop}

\begin{proof}
Recall that $\varphi_X(u) = \E[norm]{\e^{\i uX}}$ uniquely determines
the distribution of $X$.
So it is enough to show (\textast)
for $X \sim \Ndist{m}{\sigma^2}$.
Since we have the regularity
$\varphi_{X+m}(u) = \E[norm]{\e^{\i u(X+m)}}=\e^{\i um}\varphi_X(u)$,
it suffices to consider the case $m=0$.

By the Lebesgue differentiation theorem we have
% FIXME: I do not see these --Peter
\begin{align*}
\frac{\mathrm{d}}{\mathrm{d}u}\varphi_X(u)&=\frac{1}{\sqrt{2\pi \sigma^2}} \int \i x\e^{\i ux}\e^{-\frac{x^2}{2 \sigma^2}}~\mathrm{d}x\\
&=\frac{1}{\sqrt{2 \pi \sigma^2}} \int \i (\i u \e^{\i ux})\sigma^2 \e^{-\frac{x^2}{2 \sigma^2}}~\mathrm{d}x\\
&=-u \sigma^2 \varphi_X(u),
\end{align*}
and $\varphi_X(0)=1$.
Hence, $h(u)=\ln(\varphi_X(u))$ solves the ODE
\begin{align*}
	h'(u) &= \frac{\varphi_X'(u)}{\varphi_X(u)}=-u \sigma^2\\
	h(0)  &= 0,
\end{align*}
which implies $h(u)=-\frac{1}{2}u^2\sigma^2$.
\end{proof}

\begin{cor}
	For $X \sim \Ndist{0}{\sigma^2}$ and $J\in \C$
	we have $\E{\e^{JX}}=\e^{\sigma^2 \frac{J^2}{2}}$.
\end{cor}
\begin{proof}
	This follows by analytic continuation of the previous proposition.
\end{proof}

\begin{thm}\label{ddimgaussdist}
Let $X$ be $d$-dimensional Gaussian.
\begin{enumerate}[label=(\alph*)]
\item The distribution of $\X$ is uniquely determined by
	\[\m = \E{\X} = \bigl(\E[norm]{X^i}\bigr)_{1\leq i\leq d} \in \Rd,\]
	the \defn{mean vector} of $\X$,
	and
	\[
		C = (C_{ij})_{1\leq i,j\leq d}
		\quad\text{with}\quad
		C_{ij} = \Cov(X^i, X^j),
	\]
	the \defn{covariance matrix} of $\X$.
	We write $\X \sim \Ndist{\m}{C}$.
\item If $C$ is invertible,
	then the distribution of $\X$ has a Lebesgue-density
	which is given by
	\begin{align*}
		\Prob(X \in {\ud{\x}})
		&= \frac{1}{(2 \pi)^{\frac{d}{2}}}
		   \frac{1}{(\det C)^{\frac{1}{2}}}
		   \exp\bigl(-\frac{1}{2} \sp{\x-\m}{C^{-1}(\x-\m)}\bigr)
		   \ud{\x}.
	\qedhere
	\end{align*}
\end{enumerate}
\end{thm}
%TODO Kasten
\begin{proof}
\begin{enumerate}[label=(\alph*)]
\item Assume $\X$, $\Y$ are $d$-dimensional Gaussian
	with mean $\m$ and covariance matrix $C$.
	Let $\ba \in \Rd$ be arbitrary
	and set $Z \coloneqq \sum_{i=1}^d a_iX^i$
	and $W \coloneqq \sum_{i=1}^d a_iY^i$.
	Then, $Z$ and $W$ are $1$-dimensional Gaussian
	with $\E{Z} = \E{W} = \sp{\ba}{\m}$ and
\begin{align*}\label{equalityvariance}
	\V{Z} = \V{W} = \sp{a}{Ca}. \tag{\textast}
\end{align*}
So,
\begin{align*}
	\varphi_{\X}(a)
	= \E{\e^{\i \sp{\ba}{\X}}}
	= \e^{\i \sp{\ba}{\m}}\e^{-\frac{1}{2}\sp{a}{Ca}}
	= \varphi_{\Y}(a)
\end{align*}
holds for all $a\in\Rd$, which implies $\textbf{X}\sim \textbf{Y}$.
\item By \eqref{equalityvariance},
	$C$ must be positive semidefinite.
	If $C$ is invertible, $C$ must be positive definite.
	Hence, the density is well-defined.
	To check that it is the right one,
	compute its characteristic function (remains as an exercise).
	\qedhere
\end{enumerate}
\end{proof}

\begin{prop}
	Let $\X \sim \Ndist{\m}{C}$ be a $d$-dimensional Gaussian \RV
	and $A \in \R^{n\times d}$.
	Then, $A\X \sim \Ndist{A\m}{ACA^\ast}$
	where $A^\ast$ denotes the transpose of $A$.
\end{prop}
\begin{proof}
	The proof remains as an exercise.
\end{proof}

\begin{prop}
	Let $\X \sim \Ndist{\m}{C}$.
	Then $X^1, \dots, X^d$ are independent \RVs
	if and only if
	$C_{ij}=0$ for all $i\not = j$,
	\ie the pairs $X^i$, $X^j$ are uncorrelated.
\end{prop}
\begin{proof}
	The implication “$\Rightarrow$”
	always holds (if the variances exist).
	For the other direction,
	let $Y^1, \dots, Y^d$ be independent
	with $Y^i \sim \Ndist{m_i}{C_{ii}}$.
	Then by \ref{ddimgaussdist}, $\X \sim \Y$,
	which implies that the $X^i$ are independent.
\end{proof}

\begin{defi}\label{fdd}
	Let $(X_t)_{t \in T}$ be an $E$-valued \SP
	defined on a probability space $(\Omega,\mathcal{F},\Prob)$.
	The set of \defn{finite dimensional distributions} (\defn{fdd})
	of $\X$ is the family of probability measures
	\begin{align*}
		\{p_{t_1,\dots,t_n} \mid
			t_1,\dots,t_n \in T;
			% TODO: Ist diese „Injektivitätsannhme“ nötig?
			%       Für Gaussian process
			%       brauchen wir sie nämlich nicht,
			%       soweit ich das sehe. --Peter
			\text{\(t_i \neq t_j\) if \(i \neq  j\)};
			n\in \N
		\}
	\end{align*}
	where $p_{t_1,\dots,t_n} = \Prob \circ (X_{t_1},\dots,X_{t_n})^{-1}$
	is the image of $\Prob$ under $(X_{t_1},\dots,X_{t_n})$.
	In order words,
	$p_{t_1,\dots,t_n}(A_1\times \dots \times A_n)
	= \Prob(X_{t_1}\in A_1,\dots,X_{t_n}\in A_n)$
	%TODO good=measurable wrt right sigma algebra
	for all “good” sets $A_1,\dots,A_n$.
\end{defi}
%Kasten

\begin{bsp}
	Let $T=\N$, $E=\Z$ and $(X_n)_{n\in \N}$ be a simple random walk,
	this is to say $X_n=\sum_{i=1}^n Z_i$
	with $Z_i$ \iid, $\Prob(Z_i=\pm 1)=\frac{1}{2}$.
	Then
	% TODO: This does not use any of the properties specified before.
	%	This just replicates the definition.
	\begin{align*}
		p_{1,4,17}(A\times B\times C)
		&= \Prob(X_1\in A, X_4\in B, X_{17}\in C).
	\qedhere
	\end{align*}
\end{bsp}

\begin{prop}
	Let $\X$ be as in \ref{fdd}.
	Then its fdd fulfil the \defn{consistency conditions}
	that for all $t_1, \dots, t_n \in T$,
	% TODO: What is \mathcal{E}?
	$C_1, \dots, C_n \in \mathcal{E}$,
	$\sigma \in \Sym{n}$ it holds that
\begin{enumerate}[label={(C\arabic*)}]
\item $p_{t_1,\dots ,t_n}(C_1\times \dots \times C_n)
	= p_{t_{\sigma(1)}, \dots, t_{\sigma(n)}}(
		C_{\sigma(1)}\dots ,C_{\sigma(n)}
	)$
	and
\item $p_{t_1,\dots , t_n}(C_1\times \dots \times C_{n-1}\times E)
	= p_{t_1,\dots ,t_{n-1}}(C_1\times \dots \times C_{n-1})$.
\qedhere
\end{enumerate}
\end{prop}
%Kasten
\begin{proof}
	This remains as an easy exercise.
\end{proof}

\begin{defi}
	An $\Rd$-valued process $(\X_t)_{t \in T}$
	is called \defn{Gaussian process}
	if all its fdd are Gaussian measures.
\end{defi}
%Kasten

\begin{bem}
\begin{enumerate}[label=(\alph*)]
\item Explicitly, $p_{t_1,\dots , t_n}$ is Gaussian on $\R^{dn}$.
\item \nameref{counterexample_gauss_process} shows that there are processes
	where $X_t$ is Gaussian for all $t\in T$
	but where $\X$ is not a Gauss process.
	Take for example $T=\{1,2\}$, $E=\R$, $X_1=X^1$ and $X_2=X^2$.
	Morale:
	The one-dimensional distributions
	are not enough to make a process Gaussian!
\item If $\X$ is a Gauss process,
	its fdd are fully determined by
	the mean and covariance functions
	\begin{align*}
	% It's not pretty but it works
	\hspace*{2.5cm} && T &\to \Rd,& t &\mapsto \E{\X_t} && \hspace*{2.5cm}\\
	&& T^2 &\to \R^{d\times d},& \quad (s,t) &\mapsto \Cov(\X_s, \X_t).
	\end{align*}
	% TODO: Proper reference to theorem
	This follows immediately from Theorem 1.13
\qedhere
\end{enumerate}
\end{bem}

\begin{thm}
\begin{enumerate}[label=(\alph*)]
\item An $\Rd$-valued Brownian motion $\B$
	is a Gaussian process
	%TODO bereich von t?
	with $\E{\B_t}=0$ for all $t$
	and
	\begin{align*}
		\Cov(\B_s, \B_t)
		= \E{\B_s\otimes\B_t}
		% TODO: Hatten wir nicht sonst 1 \leq i,j \leq d?
		= \E{\left(B_s^iB_t^j\right)_{i,j=1,\dots,d}}
		= \min\{s,t\} \cdot \Id
	\end{align*}
	%TODO da muss noch iwo transposed hin, hier sollte es richtig sein
\item Conversely, any Gaussian process
	with the mean and covariance functions from (a)
	is a Brownian motion if it fulfills (B4).
\qedhere
\end{enumerate}
\end{thm}
%Kasten

% --- Marker --- First cleanup run currently here ---
\begin{proof}
\begin{enumerate}[label=(\alph*)]
\item Let $t_1, \dots, t_n \in \R_0^+$ with $t_1 < \dots < t_n$.
	Then with \(t_0 = 0\) and \(\mathbf D_i = \B_{t_i} - \B_{t_{i-1}}\)
	\begin{align*}
		\left(\B_{t_1}(\omega), \dots, \B_{t_n}(\omega)\right)^\top=\\
	A\left(B_{t_1}(\omega)-B_0(\omega),B_{t_2}(\omega)-B_{t_1}(\omega),\dots,B_{t_n}(\omega)-B_{t_{n-1}}(\omega)\right)^\top
	\end{align*}
	%TODO first line flushleft, second flushright
	with the lower triangle matrix
	\begin{align*}
	A = \lowtriangones
	\end{align*}
	holds.
	By (B1),(B3) and (1.9), $(B_{t_i}-B_{t_{i-1}})_{1 \leq i \leq n}\sim \Ndist{0}{C}$
	with $C_{ij}=\delta_{ij}(t_i-t_{i-1})$.
	By (1.13), $p_{t_1,\dots,t_n}\sim \Ndist{0}{ACA^\ast}$ which implies that $\textbf{B}$ is a Gaussian process.
	Now, we compute the covariance and assume $s<t$. Then we have
	\begin{align*}
	\Cov(B_s,B_t)&=\E{B_s\otimes B_t}=\E{B_s\otimes (B_t-B_s)} + \E{B_s \otimes B_s}\\
	&=s \Id=\min\{s,t\}\Id.
	\end{align*}
\item We check that (B0)-(B2) hold, as (B3),(B4) holdn by assumption.
(B0) follows from $\V{B_0}=0$ and $\E{B_0}=0$.
For (B1) and (B2) let $0<t_1<\dots<t_n$.
The covariance matrix $(B_{t_1},\dots,B_{t_n})$ is
\begin{align*}
M=(m_{ij})_{i,j\in \N}=(t_{\min\{i,j\}})_{i,j \in \N}
\end{align*}
and with $A$ as in a).
Then, $(B_{t_1}-B_0,B_{t_2}-B_{t_1},\dots,B_{t_n}-B_{t_{n-1}})$ has covariance matrix
\begin{align*}
M'=A^{-1}M\left(A^{-1}\right)^\ast=\operatorname{diag}(t_1,t_2-t_1,\dots,t_n-t_{n-1}).
\end{align*}
which implies that (B1) and (B2) holds. \qedhere
\end{enumerate}
\end{proof}

\begin{prop}
Let $\textbf{B}^1$,\dots,$\textbf{B}^d$ be $1$-dimensional Brownian motions and
let the $(\textbf{B}^i)_{i=1,\dots,d}$ be independent (as stochastic processes).
Then $\left(B_t^1,\dots,B_t^d\right)_{t\geq 0}$ is a $d$-dimensional Brownian motion.
Conversely, the coordinate processes $\left(B_t^i\right)_{t \geq 0}$ of a $d$-dimensional Brownian motion are independent $1$-dimensional Brownian motions.
\end{prop}
\begin{proof}
This is remains as an exercise or can be found in Section 2.3 of Schilling/Partzsch.
\end{proof}

\begin{prop}
Let $\textbf{B}$ be a $1$-dimensional Brownian motion. Then its fdd are given by
\begin{align}
p_{t_1,\dots ,t_n}(A_1\times \dots \times A_n) = \Prob\left(B_{t_1\in A_1,\dots,B_{t_n}\in A_n}  \right) \nonumber \\
=\frac{1}{(2\pi)^{\frac{n}{2}}}\frac{1}{\left[\prod_{j=1}^n(t_j-t_{j-1})\right]} \int_{A_1\times \dots \times A_n} \exp\left(-\frac{1}{2}\sum_{j=1}^n \frac{(x_j-x_{j-1})^2}{t_j-t_{j-1}}\right)~\mathrm{d}x
\end{align}
%TODO gleichung iwie schoen machen
for all $0=t_0<t_1<\dots < t_n$, $A_1, \dots , A_n \in  \Borel(\R), n\in \N$ with $x_0=0$ and $x=(x_1,\dots , x_n)$.
\end{prop}
\begin{proof}
Referring to Thm 1.20 this remains as an exercise.
\end{proof}

\begin{prop}
The family of fdd given in the previous proposition is consistent in the sense of (1.17),(C1) and (C2).
%TODO label hinmachen
\end{prop}

So, Brownian motions have a chance to exist. We now that it does. Nevertheless, this will take a while.

\begin{defi}
Let $(E,\mathcal{E})$ be a measurable space and $T$ a set.
\begin{enumerate}[label=\roman*)]
\item The map 
\begin{align*}
pi_t&\colon E^T\to E\\
(e_s)_{s\in T} &\mapsto e_t
\end{align*}
is called \emph{coordinate projection} to the $t$-th coordinate.
When we identify $E^T$ with $\{f \colon T \to E\}$ then $\pi_t(e)=e_t$ is the point evaluation of the function $e$ at point $t$.
\item The $\sigma$-algebra $\mathcal{E}^{\otimes T}$ is the smalles $\sigma$-algebra on $E^T$ so that all maps $\pi_t$ are $\mathcal{E}^{\otimes T}$-$\mathcal{E}$-measurable.
\item The measurable space $(E^T,\mathcal{E}^{\otimes T})$ is the \emph{canonical space} for $E$ valued stochastic processes with index set $T$.
\item If $\Omega_0\subset E^T$ is any subset (not necessarily measurable), the $\sigma$-algebra
\begin{align*}
\mathcal{E}^{\otimes T}\cap \Omega_0 \coloneqq \{A \cap \Omega_0 \colon A \in \mathcal{E}^{\otimes T}\}
\end{align*}
is called the \emph{trace} of $\mathcal{E}^{\otimes T}$ on $\Omega_0$.
The measurable space $(\Omega_0,\mathcal{E}^{\otimes T}\cap \Omega_0)$
is the canonical space for $E$-valued process with sample paths in $\Omega_0$.
\end{enumerate}
\end{defi}

\begin{bsp}
$E=\Rd$, $T=\R_0^+$ and $\Omega=E^T=\{\omega \colon \R_0^+ \to \Rd\}$,
$\pi_t(\omega)=\omega(t)$, $\Omega$=space of all \glqq paths\grqq\, $t \to \omega(t)$.
Write $X_t(\omega)=\pi_t(\omega)=\omega(t)$.
We consider $\Omega \cap C_0(\Rd)=\{\omega \in C(\R_0^+,\Rd), \omega(0)=0\}$ and $\mathcal{F}=\Borel(\Rd)^{\otimes \R_0^+} \cap C_0(\Rd)$.
Then $(C_0(\Rd),\mathcal{F})$ is the canonical measurable space for a stochastic process with continuous paths.
\end{bsp}
%TODO improve language here, find right C for continouous fcts.

\begin{bem}
The metric of \emph{local uniform convergence} on $C_0(\Rd)$ is given by 
\begin{align*}
\rho &\colon C_0\times C_0 \to \R_0^+\\
(f,g) &\mapsto \sum_{n=1}^\infty \min\{1,\sup_{0\leq t \leq n}\abs{f(t)-g(t)}\} 2^{-n}. 
\end{align*}
The Borel-$\sigma$-algebra $\Borel(C_0)$ on $C_0$ is the smallest $\sigma$-algebra on $C_0$ such that all $\rho$-open sets are measurable.
We have $\Borel(C_0)=\Borel(\Rd)^{\otimes \R_0^+}\cap C_0$.
\end{bem}
\begin{proof}
This remains as an exercise.
\end{proof}

\begin{lem}
Let $(E,\mathcal{E})$ be a measurable space, $T$ a set and $A \subseteq E^T$. Then $A \in \mathcal{E}^{\otimes T}$ if and only if there exists $I\subseteq T$ countable with $A \in \{\pi_t\colon t\in I\}$.
\end{lem}
\begin{proof}
This remains as an exercise (on some exercise sheet).
\end{proof}
%important
Recall the following:
\begin{thm}[Thm 3.29 from Probability Theory, Winter Term 17/18]
Let $(\Omega,\mathcal{F})$ be a Borel space and $\Prob$ a probability measure on $(\Omega,\mathcal{F})$.
Then for each $\sigma$-algebra $\mathcal{G} \subseteq \mathcal{F}$, a map
$\mu \colon \Omega \times \mathcal{F} \to [0,1]$ with the properties
\begin{enumerate}[label=(\roman*)]
\item $\mu(\cdot,A)$ is $\mathcal{F}$-measurable for all $A \in \mathcal{F}$
\item $\mu(\omega,\cdot)$ is a probability measure for all $\omega \in \Omega$
\item Moreover, $\mu(\omega,\cdot)$ is a conditional probability of $A$ given $\mathcal{G}$, \ie $\mu(\omega,A)=\Prob(A\mid \mathcal{G})(\omega)$ for $\Prob$-almost all $\omega \in \Omega$.
\end{enumerate}
exists. $\mu$ is called \emph{regular conditional probability}.
\end{thm}
\begin{proof}
Will be uploaded in the notes (to be done later).
\end{proof}

\begin{lem}
Let $(\Omega,\mathcal{F})$ be a Borel space, $\mu \colon \Omega \times \mathcal{F} \to [0,1]$ with properties (1.28)(i),(ii) [called a \emph{probability kernel}].
Then there exists a $\mathcal{U}[0,1]$-RV $Y$ and an $\mathcal{F} \otimes \Borel([0,1])$-measurable function
$f \colon \Omega \times [0,1] \to \Omega$ with
\begin{align*}
\mu(\omega,A)=\Prob(f(\omega,Y)\in A)= \int_0^1 \mathds{1}_{\{f(\omega,\cdot) \in A\}}(u)~\mathrm{d}u
\end{align*}
for all $\omega \in \Omega$, $A \in \mathcal{F}$.
\end{lem}
\begin{proof}
This remains as an exercise or can be found in Kallenberg [Foundations of Modern Probability, 3.22].
\end{proof}

\begin{thm}
Let $(E,\mathcal{E})$ be Borel.
For each $n \in \N$ let $\Prob_n$ be a probability measure on $(E^n,\mathcal{E}^{\otimes n})$ and assume \emph{consistency}, \ie for all
$n \in \N$ and all $A \in \mathcal{E}^{\otimes n}$ it holds that
\begin{align*}
\Prob_{n+1}(A\times E)=\Prob_n(A).
\end{align*}
Let $(\Omega,\mathcal{F},\Prob)=\left([0,1]^\N,\Borel([0,1])^{\otimes \N},\mathcal{U}([0,1])^{\otimes \N}\right)$.
Then there exist random variables $X_i \colon \Omega \to E$, $i \in \N$ such that for all $n \in \N$ and all $A \in \mathcal{E}^{\otimes n}$ it holds that
\begin{align*}
\Prob_n(A)=\Prob((X_1,\dots,X_n)\in A).
\end{align*}
\end{thm}

\begin{proof}
1. Fix $n \in \N$.
Then $(E^{n+1},\mathcal{E}^{\otimes n+1})$ is Borel as a product of Borel spaces (exercise!).
We set $\mathcal{G}_n \coloneqq \sigma (\{A_1\times \dots \times A_n \times E  \colon A_i \in \mathcal{E}\})$ and $\mu_n \colon E^{n+1}\times \mathcal{E}^{\otimes n+1}\to  [0,1]$ as in (1.28),
%TODO check if that is the right number
\ie $\mu_n(\textbf{x},A)=\Prob_{n+1}(A \mid \mathcal{G}_n)(x)$ almost surely with respect to $\P_{n+1}$ for all $A \in \mathcal{E}^{\otimes n+1}$.
Since $\textbf{x} \mapsto \mu_n(\textbf{x},A)$ is measurable with respect to $\mathcal{G}_n$, it depends only on $x_1,\dots,x_n$ and not on $x_{n+1}$.
Write
\begin{align*}
\tilde{\mu_n}((x_1,\dots,x_n),A)=\mu_n(\textbf{x},A).
\end{align*}
Note that $\mu_0(\textbf{x},A)=\Prob_1(A)$ does not depend on $x_1$.
%2. Write $\textbf{\omega}\in \Omega$ as $\textbf{\omega}=(\omega_1,\omega_2,\dots)$ and let $Y_i(\textbf{\omega})\coloneqq \omega_i$.
%Then the random variables $(Y_i)$ are iid, $\mathcal{U}[0,1]$-distributed random variables.
3. By (1.29), there exist functions $f_n \colon E^n \times [0,1]\to E$ with $\mu_n(\textbf{x},A)=\Prob(f(x_1,\dots,x_n,Y_{n+1})\in A)$.
%TODO check number
In particular
\begin{align*}
\mu_0(\textbf{x},A)=\tilde{\mu_0}(A)=\Prob(f_0(Y_1)\in A).
\end{align*}
Now, we will proceed by induction.
Put $X_1=f(Y_1)$.
Assume that $(X_1,\dots,X_n)$ have been constructed.
We set $X_{n+1}(\omega)\coloneqq f_n((X_1,(\omega),\dots,X_n(\omega),Y_{n+1}(\omega))$.
Since $\mu_{n}(\textbf{x},A)=\tilde{\mu}_{n}(x_1,\dots,x_n),A)=\Prob(f_{n+1}(x_1,\dots,x_n,Y_{n+1})\in A)$,
we find that for all $A_1,\dots,A_{n+1}\in \mathcal{E}$, it holds that
\begin{align*}
\Prob(X_i \in A_i \forall i \leq n+1)&=\E{\Prob(X_{n+1}\in A_{n+1}\mid \mathcal{G}_n)\prod_{i=1}^n \mathds{1}_{\{x_i \in A_i\}}}\\
&=\E{\Prob(f_{n}((X_1,\dots,X_n),Y_{n+1})\in A_{n+1}\mid \mathcal{G}_n)\prod_{i=1}^n \mathds{1}_{\{X_i \in A_i\}}}\eqqcolon (\ast).
\end{align*}
%TODO see how he uses \ast and maybe simply give the equation a number
Since $Y_{n+1} \amalg (X_1,\dots,X_n)$, Proposition 3.23 from Probability Theory implies that
\begin{align*}
\Prob(f_n((X_1,\dots,X_n),Y_{n+1})\in A_{n+1}\mid \mathcal{G}_n)(\omega)&=\Prob(f_n((X_1(\omega),\dots,X_n(\omega)),Y_{n+1})\in A_{n+1})\\
&=\tilde{\mu_n}((X_1(\omega),\dots,X_n(\omega)),A_{n+1})
\end{align*}
holds $\Prob$-almost surely.
%TODO overfull box 
So using the image measure we have
\begin{align*}
(\ast)&=\E{(\tilde{\mu_n}((X_1,\dots,X_n),A_{n+1})\prod_{i=1}^n \mathds{1}_{\{X_i \in A_i\}}}\\
&=\int \tilde{\mu_n}((x_1,\dots,x_n),A_{n+1})\prod_{i=1}^n \mathds{1}_{\{x_i \in A_i\}}\Prob_n(\mathrm{d}\textbf{x})\\
&=\int \tilde{\mu_n}((x_1,\dots,x_n),A_{n+1})\prod_{i=1}^n \mathds{1}_{\{x_i \in A_i\}} \Prob_{n+1}(\mathrm{d}\textbf{x})\\
&=\E{\Prob_{n+1}(A_{n+1}\mid \mathcal{G}_n)\prod_{i=1}^n \mathds{1}_{A_i}}\\
&=\E{\prod_{i=1}^{n+1} \mathds{1}_{A_i}}=\Prob_{n+1}(A_1,\times \dots \times A_{n+1}).
\end{align*}
%TODO underbrace im dritten term \mu_1(\textbf{x},A))
Then the claim follows by induction.
\end{proof}
%TODO find a good way for enumerating this proof

\begin{thm}[Kolmogorov 1932]
Let $(E,\mathcal{E})$ be Borel, $T$ a set. Let $\{p_{t_1,\dots,t_n} \colon t_1,\dots,t_n \in T,n \in \N\}$ be a family
of probability measures, that are consistent, \ie fulfill (C1),(C2) of (1.17).
%TODO rearrange sentence such that there is no overfull box anymore
Then there exists a probability measure $\Prob$ on $(E^T,\mathcal{E}^{\otimes T})$ such that
\begin{align*}
p_{t_1,\dots,t_n}(A)=\Prob((\Pi_{t_1},\dots,\Pi_{t_n})\in A)
\end{align*}
holds for all $A\in \mathcal{E}^{\otimes T}$ and all $t_1,\dots,t_n, n\in \N$.
\end{thm}

\begin{proof}
Let $A \in \mathcal{E}^{\otimes T}$.
By Lemma 1.27, there exists a countable subset $I \subset T$ with $A \in \sigma(\Pi_t \colon t \in I)$.
We write $A)B \times E^{T \setminus I}$ for some $B \in \mathcal{E}^{\otimes I}$.
By the previous theorem, there exists a unique probability measure $\Prob_I$ on $\mathcal{E}^{\otimes I}$ with
\begin{align*}
p_{t_1,\dots,t_n}(A_1\times \dots \times A_n)=\Prob_I(\Pi_{t_i} \in A_i \forall i \leq n)
\end{align*}
for all $A_1,\dots,A_n,t_1,\dots,t_n$ and $n \in \N$.
\end{proof}
Define $\Prob(A)=\Prob_I(B)$ if $A=B \times E^{T\setminus I}$ for some countable $I\subseteq I$ and some $B \in \mathcal{E}^{\otimes I}$.
By consistency, $\Prob$ is well-defined and finitely additive.
For $\sigma$-additivity, let $(A_n)_{n\in \N}\subseteq \mathcal{E}^{\otimes T}$ be disjoint.
Then for each $n$ there exists a countable set $I_n \subseteq T$ and $B_n \in \mathcal{E}^{\otimes T_n}$ with $A_n=B_n \times E^{T\setminus I_n}$.
Set $I=\bigcup_{n\in \N}I_n$ the we have $A_n=\tilde{B}_n \times E^{T \setminus I}$ for all $n$ and the $\tilde{B}_n$ are in $\mathcal{E}^{\otimes I}$ and are disjoint.
Thus,
\begin{align*}
\Prob\left(\cup_{n \in \N}A_n\right)=\Prob_I \left( \cup_{n\in \N} \tilde{B}_n\right)=\sum_{n=1}^\infty \Prob_I(\tilde{B}_n)=\sum_{i=1}^n \Prob(A_1),
\end{align*}
and we are done.

\begin{cor}
An $\Rd$-valued stochastic process fulfilling (B0)-(B3) from (1.4) exists.
Explicitly, there exists a unique probability measure $\mathcal{W}$
%TODO math...?
on $\left(\left(\Rd\right)^{\R_0^+},\left(\Borel(\Rd)\right)^{\otimes \R_0^+}\right)$ such that the random variables
\begin{align*}
B_t \colon \Omega &\to \Rd\\
\omega &\mapsto B_t(\omega)\coloneqq \omega(t)
\end{align*}
fulfill (B0)-(B3). $\mathcal{W}$ is called \emph{pre-Wienermeasure}
\end{cor}
%TODO and also named definition
It remains to see (B4).
Note that the statement $\mathcal{W}(C(\R^+,\Rd))=1$ makes no sense, as $C(\R^+,\Rd)\not \in \mathcal{F}$. We need to be more careful.

\begin{defi}
Let $D\subseteq \R_0^+$ be open and $\alpha>0$.
A function $f \colon \R_0^+\to \Rd$ is called \emph{$\alpha$-Hoelder continuous on $D$} if the Hoelder-seminorm is finite.
$f$ is locally $\alpha$-HC on $D$ if it is Hoelder-continuous on $D$ if it is Hoelder-continuous on each $D \cap [0,n]$.
We write $f \in C^\alpha(D)$ or $f \in C_{loc}^\alpha(D)$
\end{defi}
%TODO mathoperator for loc and continuous functions

\begin{bem}
\begin{enumerate}[label=\alph*)]
\item If $D$ has no cluster points, any function on $D$ is an element of $C_{loc}^\alpha$.
\item Usually, $D$ is a dense subset of some interval $[a,b]\subseteq \R_0^+$.
\item In the case of b), $f \in C^\alpha(D)$ can be uniquely extended to $[a,b]$ by the usual extension.
Uniqueness follows from the continuity.
\item If $D$ is dense and $f \in C_{loc}^\alpha(D)$ for some $\alpha>1$, then $f$ is constant.
\item $f \in C_{loc}^1(\R)$ if and only if $f$ is locally Lipschitz.
\item $\norm{\cdot}_{D,\alpha}$ is only a seminorm, as it does not detect constants.
%unterscheiden constants from 0
\item The map $\norm{\cdot}_{D,\alpha}$ is $\Borel(\Rd)^{\otimes \R_0^+}$-$\Borel([0,\infty])$-measurable if $D$ is countable.
\end{enumerate}
\end{bem}

\begin{thm}
Let $(X_t)_{t\in [0,T]}$ be an $\Rd$-valued stochastic process.
Assume that there exist $q\geq 2$, $\beta >\frac{1}{q}$, $C<\infty$ such that
\begin{align}\label{eq:condition136}
\E{\abs{X_{s,t}}^q} \leq C \abs{t-s}^{\beta q}
\end{align}
%TODO better label
holds for all $s,t \in [0,T]$ with $\abs{t-s}<\frac{1}{2}$.
Let $D\coloneqq\{k \cdot 2^{-n} \colon k,n \in \N\} \cap [0,T]$ (dyadic rationals).
Then
\begin{align*}
\E{\norm{X}_{D,\alpha}^q}<\infty
\end{align*}
holds for all $\alpha \in [0,\beta-\frac{1}{q})$.
\end{thm}
%TODO alpha=0?
\begin{proof}
Let $D_n \coloneqq \{k \cdot 2^{-n}\} \cap [0,T]$. Then we have $D=\bigcup_{n\in \N}D_n$.
%remark: dn finite
We define
\begin{align*}
K_n(\omega) \coloneqq \max\{\abs{X_{t,t+2^{-n}}(\omega)}\colon t \in D_n\}.
\end{align*}
Then by \eqref{eq:condition136} it holds that
\begin{align*}
\E{K_n^q} &\leq \E{\sum_{t \in D_n} \abs{X_{t,t+2^{-n}}}^q}\\
&\leq \abs{D_n}C (2^{-n})^{\beta q}\leq T 2^n C (2^{-n})^{\beta q}\\
&=C T (2^{-n})^{\beta q-1}.
\end{align*}
%gleichung stern
Now let $s<t$, $s,t\in D$. If $\abs{t-s}>\frac{1}{2}$ we pick $t_1,\dots,t_m\in D$ with $t_0=s$, $t_m=t$ and $\abs{t_i-t_{i-1}}<\frac{1}{2}$ for all $i$.
Since $\abs{X_t-X_s} \leq \sum_{i=1}^m \abs{X_{t_i}-X_{t_{i-1}}}$, we find
\begin{align*}
\frac{\abs{X_t-X_s}}{\abs{t-s}^\alpha} &\leq \sum_{i=1}^n \frac{\abs{X_{t_i}-X_{t_{i-1}}}}{\abs{t_i-t_{i-1}}^\alpha}\\
&\leq m \sup\{\frac{\abs{X_t-X_s}}{\abs{t-s}} \colon s,t \in D, \abs{t-s}<\frac{1}{2}\}.
\end{align*}
So it suffices to consider $\abs{t-s}\leq \frac{1}{2}$.
Then there exists $j \in \N$ with $2^j<t-s \leq 2^{-j-1}$ and $N \in \N$ so that $s,t \in D_N$.
We define $A_j\colon D_j \cap (s,t)$, then $\abs{D_j} \in \{1,2\}$.
Pick $t_j^- \coloneqq \min A_j$ as well as $t_j^+ \coloneqq \max A_j$ and set
$A_{j+1}^- \cap [s,t_j^-)$ then $\abs{A_{j+1}^-}\in \{0,1\}$.
Pick $t_{j+1}^-\coloneqq \min\{t_j^{-1}, \inf A_{j+1}^-\}$.
Analogously for $t_{j+1}^+$ via $A_{j+1}^+$ with $\min$ and $\inf$ replaced by $\max$ and $\sup$.
Inductively, $A_{j+l}^- \coloneqq D_{j+l}\cap [s,t_{j+l-1}^-)$, $t_{j+l}^-\coloneqq \min\{t_{j+l-1},\inf A_{j+l}^-$, $A_{j+l}^+\coloneqq D_{j+l} \cap (t_{j+l-1}^+,t]$, $t_{j+l}^+ \coloneqq \max\{t_{j+l-1}^+,\sup A_{j+l}^+\}$.
%TODO in align machen
This will stop when $j+l=N$ with $t_N^-=s,t_N^+=t$.
Now,
\begin{align*}
\abs{X_{s,t}} &\leq \sum_{l=0}^{N-j} \abs{X_{t_{j+l}^-}(\omega)-X_{t_{j+l-1}^-}(\omega)}+ \sum_{l=0}^{N-j} \abs{X_{t_{j+l}^+}(\omega)-X_{t_{j+l-1}^+}(\omega)}
\end{align*}
%der term bekommt \ast \ast
Each term is either equal to $0$ or the difference of some
$\abs{X_{t_{j+l}^\pm}-X_{t_{j+l}^\pm+2^{-(j+l)}}}$.
So, $(\ast \ast)\leq 2 \sum_{l=0}^{N-j} K{j+l}(\omega)\leq 2 \sum_{l=j}^\infty K_l(\omega)$.
Since we assumed $\abs{t-s}>2^{-j}$, we get
\begin{align*}
\frac{\abs{X_{s,t}(\omega)}}{\abs{t-s}^\alpha} &\leq 2^{j\alpha} \cdot 2 \sum_{l=j}^\infty K_l(\omega)\\
&\leq 2 \sum_{l=0}^\infty 2^{l\alpha} K_l(\omega).
\end{align*}
The right hand side above does not depend on $s$ and $t$.
Thus,
\begin{align*}
\E{\norm{X}_{D,\alpha}^q}^{\frac{1}{q}} &\leq \E{(2T)^q (2 \sum_{l=0}^\infty 2^{l\alpha} K_l)^q}^\frac{1}{q}\\
&=4T \abs{\sum_{l=0}^\infty 2^{l\alpha} K_l}_{\mathit{L}^q} \leq 4T \sum_{l=0}^\infty 2^{l\alpha} \abs{K_l}_{\mathit{L}^q}\\
&\leq 2CT \sum_{l=0}^\infty 2^{l\alpha} 2^{-l \frac{\beta q -1}{q}}
\end{align*}
where we used $(\ast)$ in the last step.
%mathit richtig?
For $\alpha <\beta -\frac{1}{q}$ this sum is finite.
\end{proof}

\begin{cor}
Let $(X_t)_{t\in \R_0^+}$ be an $\Rd$-valued stochastic process on $(\Omega,\mathcal{F},\Prob)$ which fulfills \eqref{eq:condition136} for all $T$.
\begin{enumerate}[label=\alph*)]
\item(Kolmogorov-Chentsov-Than) There exists a stochastic process $(\tilde{X}_t)_{t \in \R_0^+}$ on $(\Omega,\mathcal{F},\Prob)$ such that
\begin{enumerate}[label=\roman*)]
\item for all $\omega \in \Omega$ the map $t \mapsto \tilde{X}_t(\omega)$ is continuous, we say that $\tilde{X}$ has continuous paths.
\item for all $ \in \R_0^+$ we have $\Prob(X_t=\tilde{X}_t$, this is to say $\tilde{X}$ is a \emph{version} of $X$.
\end{enumerate}
%namen richtig?!
\item On the canonical space $(C(\R_0^+,\Rd),\mathcal{F}_{can})$
%TODO can im textmodus
there exists a probability measure $\Prob_0$ so that with
\begin{align*}
Y_t(\hat{\omega)}=\hat{\omega}(t)~~\forall \hat{\omega} \in C(R_0^+,\Rd)
\end{align*}
the processes $(X_t)$ and $(Y_t)$ have the same fdd.
\end{enumerate}
\end{cor}

\begin{proof}
\begin{enumerate}[label=\alph*)]
By Thm 1.36 we have $\E{\norm{X}_{D_T,\alpha}^q}<\infty$ for some $\alpha>0,q\geq 2$ and all $D_T=D_{\R_0^+}\cap [0,T]$.
Hence $\Prob(\norm{X}_{D_T,\alpha}<\infty)=1$ for all $T$ and therefore $\Prob(X\in C_{loc}^\alpha (D))=1$.
The set $\{\omega \in \Omega \colon X(w) \in C_{loc}^\alpha(D))\eqqcolon \Omega_0$ depends on countably many indices and is thus measurable.
Define
\begin{align*}
\tilde{X}_t(\omega)\coloneqq \begin{cases}
\lim_{t_n \to t} X_{t_n}(\omega),~~&\text{for } t_n \in D, w \omega \in \Omega_0,\\
0, &\text{otherwise }.
\end{cases}
\end{align*}
$\tilde{X}_t(\omega)$ is independent of the approximating sequence and $t \mapsto \tilde{X}_t(\omega)$ is continuous for all $\omega \in \Omega$.
Hence (i) holds. For (ii), let
\begin{align*}
Z_t(\omega)\coloneqq X_t(\omega) -\tilde{X}_t(\omega)\\
Z_{t_n} \coloneqq X_t(\omega)-\tilde{X}_{t_n}(\omega)
\end{align*}
for $t_n \in D$, $t_n \to t$.
Then $Z_{t_n}(\omega)\to Z_t(\omega)$ on $\Omega_0$, \ie almost surely, and by \eqref{eq:condition136}
\begin{align*}
\Prob(\abs{Z_{t_n}}>\varepsilon)&\leq \frac{1}{\varepsilon^q}\E{\abs{X_{t_n}-X_t}^q}\\
&\leq C \frac{1}{\varepsilon^q}\abs{t_n-t}^{\beta q} \to 0,
\end{align*}
$Z_{t_n}\to 0$ in probability and hence $Z_t=0$ almost surely.
\item The map 
\begin{align*}
F \colon \Omega &\to \Omega_0\\
\omega &\mapsto (\tilde{X}_t(\omega))_{t\in \R_0^+}
\end{align*}
is measurable.
Then $\Prob_0=\Prob \circ F^{-1}$. \qedhere
\end{enumerate}
\end{proof}

\begin{thm}
\begin{enumerate}[label=\alph*)]
\item There exists a stochastic process satisfying B0-B4, \ie a Brownian motion.
\item There exists a probability measure $\mathcal{W}$ on $(C_0,\mathcal{F})$ such that under $\mathcal{W}$ the projection $\omega \mapsto \pi_t(\omega)=B_t(\omega)=\omega(t)$ form a Brownian motion.
The space $(C_0,\mathcal{F},\mathcal{W})$ is called \emph{Wiener space}.
\end{enumerate}
\end{thm}

\begin{proof}
\begin{enumerate}[label=(\roman*)]
\item By 1.22 we can restrict to 1-d BM.
By Cor 1.33 a process $\tilde{B}_t)$ with B0-B3 exists. By B3
\end{enumerate}
\end{proof}
