\chapter{Brownian Motion}
\begin{defi}
Let $(E, \Sigma)$ be a measurable space and $T$ a set.
A collection of $(E, \Sigma)$-valued random variables (RVs)
$\X = (X_t)_{t\in T}$ is called \defn{$E$-valued stochastic process (SP) with index set $T$}.
\end{defi}
%TODO Kasten

\begin{bsp}
\begin{enumerate}[label=(\alph*)]
\item An SP  with $(E, \Sigma) = (\Rd, \Borel(\Rd))$
	and $T=\R_{\geq 0}$
	is called \defn{$\Rd$-valued, continuous-time SP}.
\item For $E = \{-1,1\}$, \(\Sigma = \Po(E)\) and $T=\Z^d$,
	the SP is called \defn{spin system}.
\item If $E$ is countable and $T=\N_0$, we speak of a \defn{time discrete SP}.
\end{enumerate}
\end{bsp}

From a dynamical point of view,
$X_t$ is a $t$-dependent quantity that changes with time.
This perspective is suitable for the comprehension of the first and third example.
From a global point of view,
an SP is a single RV with values in the space $\Omega=E^T=\{f\colon T\to E\}$.
In the first example,
this means to consider the whole \defn{path} $(X_t)_{t\geq 0}$ as one object.
In the second example,
each spin configuration in $\{-1,1\}^{\Z^d}$ is an element of $\Omega$.
This raises some questions:
What is the “right” $\sigma$-algebra on \(\Omega\)?
Does it even exist?

\begin{defi}
Let $\X = (X_t)_{t\in T}$ be an SP
where the state space $E$ is a group, \eg $E=\Rd$,
and $T\subseteq \R$.
The family $(X_{s,t})_{s,t\in T}$ with $X_{s,t}\coloneqq X_t-X_s$
is called the \defn{increment process of $\X$} or \defn{set of increments}.

An SP has \defn{independent increments}
if for all $n\in \N$ and all
$s_1 < t_1 \leq s_2 < t_2 \leq \dots \leq s_n < t_n$
with $s_i, t_i \in T$,
the RVs $(X_{s_i,t_i})_{1\leq i \leq n}$ are independent.

An SP has \defn{stationary increments}
if for all $n\in \N$, all \(r \in T\) and all
$s_1 < t_1 \leq s_2 < t_2 \leq \dots \leq s_n < t_n$
with $s_i,t_i \in T$,
we have
\begin{align*}
(X_{s_i,t_i})_{i=1,\dots,n}\sim  (X_{s_i+r,t_i+r})_{i=1,\dots,n},
\end{align*}
\ie that the increments are equal in distribution
regardless at which time we started looking.
\end{defi}
%TODO Kasten

\begin{defi}
An $\Rd$-valued SP $\B=(B_t)_{t \in \R_{\geq 0}}$
is called \emph{Brownian Motion} (BM) if
\begin{enumerate}[label=(B\arabic*)]
\item $B_0(\omega)=0$ for almost all $w\in \Omega$,
\item $\B$ has independent increments,
\item $\B$ has stationary increments,
\item the increments are normally distributed, \ie
	\[B_{s,t} \coloneqq B_t - B_s \sim B_{t-s}\sim \Ndist{0}{(t-s)\Id},\]
	and
\item the map $t\mapsto B_t(\omega)$ is continuous for all $\omega \in \Omega$.
\end{enumerate}
\end{defi}
%TODO Kasten

\begin{bem}
Checking the requirements (B0)\,--\,(B4)
in view of $B_t=\int_0^t \xi_s~\mathrm{d}s$:
\begin{enumerate}[label=(B\arabic*)]
\item $\int_0^0\xi_s~\mathrm{d}s=0$
\item $B_t-B_s=\int_s^t \xi_r~\mathrm{d}r$
	and $\xi_r \amalg (\xi_s)_{s\not =r}~~\forall r$
% TODO: Is this correct?
%	and the \(\xi_r\) are independent from each other
\item The distribution of $\xi_r$ does not depend on $r$.
\item Central limit theorem and Riemann approximation.
\item The map $t \mapsto \int_0^t f_s~\mathrm{d}s$ is continuous
	for all “sensible” functions $f$,
	in particular for $f = \xi$.
\end{enumerate}
\end{bem}

\begin{defi}
The \defn{Gaussian measure} $\Ndist{m}{\sigma^2}$
with mean $m$ and variance $\sigma^2$ is
the probability measure on $(\R,\mathcal{B}(\R))$ with Lebesgue density
\begin{align*}
	g_{m, \sigma^2}(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2\sigma^2}(x-m)^2\right).
\end{align*}
\end{defi}

\begin{prop}
Let $X\sim \Ndist{m}{\sigma^2}$.
Then:
\begin{enumerate}[label=(\alph*)]
\item $\E{X } =m$, $\V{X} = \sigma^2$
\item We have the \defn{Gaussian tail estimate}
	% TODO: Refactor terms?
	\begin{align*}
	\frac{1}{\sqrt{2\pi}} \frac{C}{C^2+1}\e^{-\frac{C^2}{2}}
	\leq
	\mathds{P}(X-m\geq C \sigma)
	\leq
	\frac{1}{\sqrt{2\pi}} \frac{1}{C}\e^{-\frac{C^2}{2}},
	\end{align*}
	for all $C>0$, $\sigma>0$.
\item For $(m_k)_{k\in \N}\subseteq \R$, $m\in \R$,
	$(\sigma_k)_{k\in \N}\subseteq \R_{\geq 0}$ and $\sigma\in \R_{\geq 0}$
	we have that
	$(m_k, \sigma_k) \to (m, \sigma)$
	if and only if
	$\Ndist{m_k}{\sigma_k^2} \distto  \Ndist{m}{\sigma^2}$.
\end{enumerate}
\end{prop}

\begin{defi}
A $\Rd$-valued RV $X$ is called \emph{$d$-dimensional Gaussian} if for all linear functionals $L\colon \Rd\to \R$ there exist $m,\sigma^2$ with $LX\sim \Ndist{m}{\sigma^2}$.
Explicitly: if $X=(X^1,\dots,X^d)$ this means that for all $a_1,\dots,a_d \in \R$ there exist $m,\sigma^2$ such that $\sum_{i=1}^d a_i X^i \sim \Ndist{m}{\sigma^2}$.
\end{defi}

\begin{bsp}
\begin{enumerate}[label=(\alph*)]
\item If $X^1,\dots,X^d$ are independent $1$-dimensional Gaussian, then 
$\textbf{X}=(X^1,\dots,X^d)$ is $d$-dimensional Gaussian.
\item \emph{Warning:} without independence, this is not true in general.
Let $X^1\sim \Ndist01$ and
\begin{align*}
X^2(\omega)=\begin{cases}
X^1(\omega),~~&\forall \omega \text{ with } \abs{X^1(\omega)}>1,\\
-X^1(\omega),&\forall \omega \text{ with } \abs{X^1(\omega)}\leq 1.
\end{cases}
\end{align*}
Then, $X^2\sim \Ndist01$ (check it, compute $\mathds{P}(X^2<c)$ for all $c\in \R$), but $(X^1,X^2)$ is not Gaussian as
$\abs{X^1(\omega)-X^2(\omega)}\leq 2$ for all $\omega \in \Omega$ and
$\abs{X^1(\omega)-X^2(\omega)}\not \equiv 0$ which implies that $X^1-X^2$ is not Gaussian.
\end{enumerate}
\end{bsp}

\begin{prop}
A real RV $X$ is $\Ndist{m}{\sigma^2}$-distributed if and only if
its characteristic function is given by
\begin{align}\label{eq:characteristicfunction}
\varphi_X(u)=\e^{\i um}\e^{-\frac{1}{2}\sigma^2u^2}.
\end{align}
\begin{proof}
Recall that $\varphi_X(u)=\E{\e^{\i uX}}$ uniquely determines the distribution of $X$.
So it is enough to show \eqref{eq:characteristicfunction} if
$X\sim \Ndist{m}{\sigma^2}$.
Since $\varphi_{X+m}\E{\e^{\i u(X+m)}}=\e^{\i um}\varphi_X(u)$,
it suffices to consider the case $m=0$.
By the Lebesgue differentiation theorem we have
\begin{align*}
\frac{\mathrm{d}}{\mathrm{d}u}\varphi_X(u)&=\frac{1}{\sqrt{2\pi \sigma^2}} \int \i x\e^{\i ux}\e^{-\frac{x^2}{2 \sigma^2}}~\mathrm{d}x\\
&=\frac{1}{\sqrt{2 \pi \sigma^2}} \int \i (\i u \e^{\i ux})\sigma^2 \e^{-\frac{x^2}{2 \sigma^2}}~\mathrm{d}x\\
&=-u \sigma^2 \varphi_X(u),
\end{align*}
and $\varphi_X(0)=1$. Hence, $h(u)=\ln \varphi_X(u)$ solves the ODE
\begin{align*}
h'(u)&=\frac{\varphi_X'(u)}{\varphi_X(u)}=-u \sigma^2\\
h(0)=0,
\end{align*}
which implies $h(u)=-\frac{1}{2}u^2\sigma^2$.
\end{proof}
\end{prop}

\begin{cor}
For $X \sim \Ndist{0}{\sigma^2}$ and $J\in \C$ we have $\E{\e^{JX}})=\e^{\sigma^2 \frac{J^2}{2}}$.
\begin{proof}
This follows by analytic continuation of the previous proposition.
\end{proof}
\end{cor}

\begin{thm}
Let $X$ be $d$-dimensional Gaussian.
\begin{enumerate}[label=(\alph*)]
\item The distribution of $X$ is uniquely determined by the \emph{mean vector of $X$} $\textbf{m}=\E{X}=\E{(X^i)}_{1\leq i\leq d} \in \Rd$ and the \emph{covariance matrix of $X$} $C=(C_{ij})_{1\leq i,j\leq d}$ with $C_{ij}=\Cov(X^i,X^j)$.
We write $X \sim \Ndist{\textbf{m}}{C}$.
\item If $C$ is invertible, then the distribution of $X$ has a Lebesgue-density and
\begin{align*}
\mathds{P}(X \in \mathrm{d}\textbf{x}))=\frac{1}{(2 \pi)^{\frac{d}{2}}} \frac{1}{(\det C)^{\frac{1}{2}}} \e^{-\frac{1}{2}(\textbf{x}-\textbf{m},C^{-1}(\textbf{x}-\textbf{m}))}~\mathrm{d}\textbf{x}.
\end{align*}
\end{enumerate}
\begin{proof}
\begin{enumerate}[label=(\alph*)]
\item Assume $X,Y$ are $d$-dimensional Gaussian with mean $\textbf{m}$ and covariance matrix $C$.
Let $a\in \Rd$, $Z=\sum_{i=1}^d a_iX^i$, $W=\sum_{i=1}^d a_i Y^i$.
Then, $Z,W$ are $1$-dimensional Gaussian with $\E{Z}=\E{W}=(a,\textbf{m})$ and
\begin{align}\label{eq:equalityvariance}
\mathds{V}(Z)=\mathds{V}(W)=(a,Ca).
\end{align}
So,
\begin{align*}
\varphi_{\textbf{X}}(a)=\E{\e^{\i (a,X)}}=\e^{\i (a,\textbf{m})}\e^{-\frac{1}{2}(a,Ca)}=\varphi_\textbf{Y}(a)
\end{align*}
holds for all $a\in\Rd$ which implies $\textbf{X}\sim \textbf{Y}$.
\item By \eqref{eq:equalityvariance}, $C$ must be positive semidefinite. If $C$ is invertible, $C$ must be positive definite.
Hence, the density is well-defined.
To check that it is the right one, compute its characteristic function (remains as an exercise). \qedhere
\end{enumerate}
\end{proof}
\end{thm}
%TODO Kasten

\begin{prop}
Let $X \sim \Ndist{\textbf{m}}{C}$ $d$-dimensional Gaussian, $A \in \R^{n\times d}$.
Then $AX\sim \Ndist{A\textbf{m}}{ACA^\ast}$ where $A^\ast$ denotes the transpose of $A$.
\begin{proof}
The proof remains as an exercise.
\end{proof}
\end{prop}

\begin{prop}
Let $\textbf{X}\sim \Ndist{\textbf{m}}{C}$.
Then $X^1,\dots,X^d$ are independent RVs if and only if $C_{ij}=0$ for all $i\not = j$, i.e., $X^i,X^j$ are uncorrelated.
\begin{proof}
\glqq $\Rightarrow$\grqq\, always holds (if the variances exist).
For the other direction let $Y^1,\dots,Y^d$ be independent with $Y^i\sim \Ndist{m_i}{C_{ii}}$.
Then by (1.13) $\textbf{X} \sim \textbf{Y}$ which implies that $(X^i)$ are independent.
%TODO label richtig machen
\end{proof}
\end{prop}

\begin{defi}
Let $(X_t)_{t \in T}$ be an $E$-valued SP defined on a probability space $(\Omega,\mathcal{F},\Prob)$. The set of \emph{finite dimensional distributions (fdd)} of $\textbf{X}$ is the family of probability measures
\begin{align*}
\{p_{t_1,\dots,t_n}\mid t_1,\dots,t_n, t_i\not =t_j \text{ if } i \not = j,n\in \N\}
\end{align*}
where $p_{t_1,\dots,t_n}=\Prob \circ (X_{t_1},\dots,X_{t_n})^{-1}$ is the image of $\Prob$ under $(X_{t_1},\dots,X_{t_n})$.
In order words, $p_{t_1,\dots,t_n}(A_1\times \dots \times A_n)=\Prob(X_{t_1}\in A_1,\dots,X_{t_n}\in A_n)$ for all \glqq good\grqq\, sets $A_1,\dots,A_n$.
\end{defi}
%Kasten
%TODO good=measurable wrt right sigma algebra

\begin{bsp}
Let $T=\N$, $E=\Z$ and $(X_n)_{n\in \N}$ be a simple random walk, this is to say
$X_n=\sum_{i=1}^n Z_i$ with $Z_i$ iid, $\Prob(Z_i=\pm 1)=\frac{1}{2}$.
Then
\begin{align*}
p_{1,4,17}(A\times B\times C)=\Prob(X_1\in A,X_4\in B,X_{17}\in C).
\end{align*}
\end{bsp}

\begin{prop}
Let $\textbf{X}$ be as in (1.16). Then its fdd fulfil the \emph{consistency conditions} that for all $t_1,\dots,t_n \in T$, $C_1,\dots,C_n \in \mathcal{E}$, $\sigma\in S_n$ it holds that
\begin{enumerate}
\item[(C1)] $p_{t_1,\dots ,t_n}(C_1\times \dots \times C_n)=p_{t_{\sigma(1)},\dots ,t_{\sigma(n)}}(C_{\sigma(1)}\dots ,C_{\sigma(n)})$,
\item[(C2)] $p_{t_1,\dots , t_n}(C_1\times \dots \times C_{n-1}\times E)=p_{t_1,\dots ,t_{n-1}}(C_1\times \dots \times C_{n-1})$.
\end{enumerate}
\begin{proof}
This remains as an easy exercise.
\end{proof}
\end{prop}
%Kasten
%TODO 1.16 ordentlich machen

\begin{defi}
An $\Rd$-valued process $(X_t)_{t \in T}$ is called \emph{Gaussian process} if all its fdd are Gaussian measures.
\end{defi}
%Kasten

\begin{bem}
\begin{enumerate}[label=(\alph*)]
\item Explicitly, $p_{t_1,\dots , t_n}$ is Gaussian on $\R^{dn}$.
\item (1.10b) shows that there are processes where $X_t$ is Gaussian for all $t\in T$ but where $\textbf{X}$ is not a Gauss process. Take for example $T=\{1,2\}$, $E=\R$, $X_1=X^1,X_2=X^2$ in (1.10).
Morale: the one-dimensional distributions are not enough to make a process Gaussian!
\item If $\textbf{X}$ is a Gauss process, its fdd are fully determined by mean function $T \to \Rd, t \mapsto \E{X_t}$ and the covariance function $T^2\to \R^{d\times d}, (s,t) \mapsto \Cov(X_s,X_t)$.
This follows immediately from Theorem 1.13
%TODO 1.13 ordentlich machen
%TODO vlt in align machen, dann sieht es schoener aus.
\end{enumerate}
\end{bem}
%TODO 1.10 ordentlich machen

\begin{thm}
\begin{enumerate}[label=(\alph*)]
\item An $\Rd$-valued Brownian motion $\textbf{B}$ is a Gaussian process with $\E{B_t}=0$ for all $t$.
%TODO bereich von t?
and 
\begin{align*}
Cov(B_s,B_t)=\E{B_s\otimes B_t}=\E{\left(B_s^iB_t^j\right)_{i,j=1,\dots,d}}=\min\{s,t\} \cdot I_{\Rd}
\end{align*}
%TODO da muss noch iwo transposed hin, hier sollte es richtig sein
\item Conversely, any Gaussian process with the mean and covariance functions from a) is a Brownian motion if it fulfills (B4).
\end{enumerate}
\begin{proof}
\begin{enumerate}[label=\alph*)]
\item Let $t_1,\dots,t_n\in \R_0^+$ with $t_1<\dots<t_n$. Then
\begin{align*}
\left(B_{t_1}(\omega),\dots,B_{t_n}(\omega)\right)^t=\\
A\left(B_{t_1}(\omega)-B_0(\omega),B_{t_2}(\omega)-B_{t_1}(\omega),\dots,B_{t_n}(\omega)-B_{t_{n-1}}(\omega)\right)^t
\end{align*}
%TODO transponiert ordentlichen machen
where $A$ denotes the \glqq right\grqq\, matrix.
%%TODO ones on lower half
By (B1),(B3) and (1.9), $(B_{t_i}-B_{t_{i-1}})_{1 \leq i \leq n}\sim \Ndist{0}{C}$
with $C_{ij}=\delta_{ij}(t_i-t_{i-1})$.
By (1.13), $p_{t_1,\dots,t_n}\sim \Ndist{0}{ACA^\ast}$ which implies that $\textbf{B}$ is a Gaussian process.
Now, we compute the covariance and assume $s<t$. Then we have
\begin{align*}
\Cov(B_s,B_t)&=\E{B_s\otimes B_t}=\E{B_s\otimes (B_t-B_s)} + \E{B_s \otimes B_s}\\
&=s \Id=\min\{s,t\}\Id.
\end{align*}
\item We check that (B0)-(B2) hold, as (B3),(B4) holdn by assumption.
(B0) follows from $\V{B_0}=0$ and $\E{B_0}=0$.
For (B1) and (B2) let $0<t_1<\dots<t_n$.
The covariance matrix $(B_{t_1},\dots,B_{t_n})$ is
\begin{align*}
M
\end{align*}
and with $A$ as in a), $A^{-1}=$.
Then, $(B_{t_1}-B_0,B_{t_2}-B_{t_1},\dots,B_{t_n}-B_{t_{n-1}})$ has covariance matrix
\begin{align*}
M'=A^{-1}M\left(A^{-1}\right)^\ast=
\end{align*}
which implies that (B1) and (B2) holds. \qedhere
\end{enumerate}
\end{proof}
\end{thm}
%Kasten

\begin{prop}
Let $\textbf{B}^1$,\dots,$\textbf{B}^d$ be $1$-dimensional Brownian motions and
let the $(\textbf{B}^i)_{i=1,\dots,d}$ be independent (as stochastic processes).
Then $\left(B_t^1,\dots,B_t^d\right)_{t\geq 0}$ is a $d$-dimensional Brownian motion.
Conversely, the coordinate processes $\left(B_t^i\right)_{t \geq 0}$ of a $d$-dimensional Brownian motion are independent $1$-dimensional Brownian motions.
\begin{proof}
This is remains as an exercise or can be found in Section 2.3 of Schilling/Partzsch.
\end{proof}
\end{prop}

\begin{prop}
Let $\textbf{B}$ be a $1$-dimensional Brownian motion. Then its fdd are given by
\begin{align}
p_{t_1,\dots ,t_n}(A_1\times \dots \times A_n) = \Prob\left(B_{t_1\in A_1,\dots,B_{t_n}\in A_n}  \right) \nonumber \\
=\frac{1}{(2\pi)^{\frac{n}{2}}}\frac{1}{\left[\prod_{j=1}^n(t_j-t_{j-1})\right]} \int_{A_1\times \dots \times A_n} \exp\left(-\frac{1}{2}\sum_{j=1}^n \frac{(x_j-x_{j-1})^2}{t_j-t_{j-1}}\right)~\mathrm{d}x
\end{align}
%TODO gleichung iwie schoen machen
for all $0=t_0<t_1<\dots < t_n$, $A_1, \dots , A_n \in  \Borel(\R), n\in \N$ with $x_0=0$ and $x=(x_1,\dots , x_n)$.
\begin{proof}
Referring to Thm 1.20 this remains as an exercise.
\end{proof}
\end{prop}

\begin{prop}
The family of fdd given in the previous proposition is consistent in the sense of (1.17),(C1) and (C2).
%TODO label hinmachen
\end{prop}

So, Brownian motions have a chance to exist. We now that it does. Nevertheless, this will take a while.

\begin{defi}
Let $(E,\mathcal{E})$ be a measurable space and $T$ a set.
\begin{enumerate}[label=\roman*)]
\item The map 
\begin{align*}
pi_t&\colon E^T\to E\\
(e_s)_{s\in T} &\mapsto e_t
\end{align*}
is called \emph{coordinate projection} to the $t$-th coordinate.
When we identify $E^T$ with $\{f \colon T \to E\}$ then $\pi_t(e)=e_t$ is the point evaluation of the function $e$ at point $t$.
\item The $\sigma$-algebra $\mathcal{E}^{\otimes T}$ is the smalles $\sigma$-algebra on $E^T$ so that all maps $\pi_t$ are $\mathcal{E}^{\otimes T}$-$\mathcal{E}$-measurable.
\item The measurable space $(E^T,\mathcal{E}^{\otimes T})$ is the \emph{canonical space} for $E$ valued stochastic processes with index set $T$.
\item If $\Omega_0\subset E^T$ is any subset (not necessarily measurable), the $\sigma$-algebra
\begin{align*}
\mathcal{E}^{\otimes T}\cap \Omega_0 \coloneqq \{A \cap \Omega_0 \colon A \in \mathcal{E}^{\otimes T}\}
\end{align*}
is called the \emph{trace} of $\mathcal{E}^{\otimes T}$ on $\Omega_0$.
The measurable space $(\Omega_0,\mathcal{E}^{\otimes T}\cap \Omega_0)$
is the canonical space for $E$-valued process with sample paths in $\Omega_0$.
\end{enumerate}
\end{defi}

\begin{bsp}
$E=\Rd$, $T=\R_0^+$ and $\Omega=E^T=\{\omega \colon \R_0^+ \to \Rd\}$,
$\pi_t(\omega)=\omega(t)$, $\Omega$=space of all \glqq paths\grqq\, $t \to \omega(t)$.
Write $X_t(\omega)=\pi_t(\omega)=\omega(t)$.
We consider $\Omega \cap C_0(\Rd)=\{\omega \in C(\R_0^+,\Rd), \omega(0)=0\}$ and $\mathcal{F}=\Borel(\Rd)^{\otimes \R_0^+} \cap C_0(\Rd)$.
Then $(C_0(\Rd),\mathcal{F})$ is the canonical measurable space for a stochastic process with continuous paths.
\end{bsp}
%TODO improve language here, find right C for continouous fcts.

\begin{bem}
The metric of \emph{local uniform convergence} on $C_0(\Rd)$ is given by 
\begin{align*}
\rho &\colon C_0\times C_0 \to \R_0^+\\
(f,g) &\mapsto \sum_{n=1}^\infty \min\{1,\sup_{0\leq t \leq n}\abs{f(t)-g(t)}\} 2^{-n}. 
\end{align*}
The Borel-$\sigma$-algebra $\Borel(C_0)$ on $C_0$ is the smallest $\sigma$-algebra on $C_0$ such that all $\rho$-open sets are measurable.
We have $\Borel(C_0)=\Borel(\Rd)^{\otimes \R_0^+}\cap C_0$.
\begin{proof}
This remains as an exercise.
\end{proof}
\end{bem}

\begin{lem}
Let $(E,\mathcal{E})$ be a measurable space, $T$ a set and $A \subset E^T$. Then $A \in \mathcal{E}^{\otimes T}$ if and only if there exists $I\subseteq T$ countable with $A \in \{\pi_t\colon t\in I\}$.
\begin{proof}
This remains as a proof (on some exercise sheet).
\end{proof}
\end{lem}
%important
Recall the following:
\begin{thm}[Thm 3.29 from Probability Theory, Winter Term 17/18]
Let $(\Omega,\mathcal{F})$ be a Borel space and $\Prob$ a probability measure on $(\Omega,\mathcal{F})$.
Then for each $\sigma$-algebra $\mathcal{G} \subseteq \mathcal{F}$, a map
$\mu \colon \Omega \times \mathcal{F} \to [0,1]$ with the properties
\begin{enumerate}[label=(\roman*)]
\item $\mu(\cdot,A)$ is $\mathcal{F}$-measurable for all $A \in \mathcal{F}$
\item $\mu(\omega,\cdot)$ is a probability measure for all $\omega \in \Omega$
\item Moreover, $\mu(\omega,\cdot)$ is a conditional probability of $A$ given $\mathcal{G}$, \ie $\mu(\omega,A)=\Prob(A\mid \mathcal{G})(\omega)$ for $\Prob$-almost all $\omega \in \Omega$.
\end{enumerate}
exists. $\mu$ is called \emph{regular conditional probability}.
\begin{proof}
Will be uploaded in the notes (to be done later).
\end{proof}
\end{thm}

\begin{lem}
Let $(\Omega,\mathcal{F})$ be a Borel space, $\mu \colon \Omega \times \mathcal{F} \to [0,1]$ with properties (1.28)(i),(ii) [called a \emph{probability kernel}].
Then there exists a $\mathcal{U}[0,1]$-RV $Y$ and an $\mathcal{F} \otimes \Borel([0,1])$-measurable function
$f \colon \Omega \times [0,1] \to \Omega$ with
\begin{align*}
\mu(\omega,A)=\Prob(f(\omega,Y)\in A)= \int_0^1 \mathds{1}_{\{f(\omega,\cdot) \in A\}}(u)~\mathrm{d}u
\end{align*}
for all $\omega \in \Omega$, $A \in \mathcal{F}$.
\begin{proof}
This remains as an exercise or can be found in Kallenberg [Foundations of Modern Probability, 3.22].
\end{proof}
\end{lem}